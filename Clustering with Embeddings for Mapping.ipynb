{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31f1f31-55c7-4cab-9cec-e84ad1886a98",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4783ea-8e58-417d-ac7f-2236d11f0056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install openai==0.28.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7eea3-7445-4817-8fd1-9c83f7c81202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install s3fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51a805-f459-487a-860c-804f8a49a4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3bbb4f-a211-4f90-b3fd-39d8a2acdc19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s3_path = 's3://ml-translation-input-dev/non_english_sample_100k.csv'\n",
    "\n",
    "df = pd.read_csv(s3_path)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587b434-eff9-4b6e-a1bc-02c9a60f24ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40991a2-35db-47ba-b694-e286094bd08e",
   "metadata": {},
   "source": [
    "# Processing Embeddings in Batches\n",
    "\n",
    "### Overview\n",
    "- **Model**: Generates embeddings using OpenAI's `text-embedding-ada-002` model.\n",
    "- **Batching**: Processes data in batches for efficiency.\n",
    "- **Output**: Saves embeddings to an S3 bucket.\n",
    "- **Progress Tracking**: Utilizes a checkpoint file to allow resuming if interrupted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec7667-2bed-453a-ad2e-60f476068ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import s3fs\n",
    "import os\n",
    "\n",
    "\n",
    "openai.api_key = \"\"\n",
    "\n",
    "batch_size = 1000\n",
    "EMBEDDING_DIM = 1536\n",
    "ZERO_EMBEDDING = [0.0] * EMBEDDING_DIM\n",
    "\n",
    "def create_embeddings_batch(texts, max_retries=3):\n",
    "    embeddings = []\n",
    "    non_empty_indices = [i for i, text in enumerate(texts) if isinstance(text, str) and text.strip()]\n",
    "    non_empty_texts = [text for text in texts if isinstance(text, str) and text.strip()]\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if non_empty_texts:\n",
    "                response = openai.Embedding.create(\n",
    "                    input=non_empty_texts,\n",
    "                    model=\"text-embedding-ada-002\"\n",
    "                )\n",
    "                batch_embeddings = [item['embedding'] for item in response['data']]\n",
    "                if len(batch_embeddings) != len(non_empty_texts):\n",
    "                    batch_embeddings = [ZERO_EMBEDDING] * len(non_empty_texts)\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error en el intento {attempt + 1}: {e}\")\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        batch_embeddings = [ZERO_EMBEDDING] * len(non_empty_texts)\n",
    "\n",
    "    full_embeddings = [ZERO_EMBEDDING] * len(texts)\n",
    "    for idx, emb in zip(non_empty_indices, batch_embeddings):\n",
    "        if len(emb) == EMBEDDING_DIM:\n",
    "            full_embeddings[idx] = emb\n",
    "        else:\n",
    "            full_embeddings[idx] = ZERO_EMBEDDING\n",
    "    return full_embeddings\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "with fs.open(input_s3_path, 'r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "if 'embedding' not in df.columns:\n",
    "    df['embedding'] = None\n",
    "\n",
    "if fs.exists(checkpoint_file):\n",
    "    with fs.open(checkpoint_file, 'r') as f_chk:\n",
    "        processed_batches = json.load(f_chk)\n",
    "else:\n",
    "    processed_batches = []\n",
    "total_batches = len(df) // batch_size + 1\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Processing Batchs\"):\n",
    "    if i in processed_batches:\n",
    "        continue  \n",
    "\n",
    "    batch_df = df.iloc[i:i + batch_size]\n",
    "    batch_texts = batch_df['JOB_TITLE'].astype(str).tolist() \n",
    "    batch_embeddings = create_embeddings_batch(batch_texts)\n",
    "\n",
    "    batch_df['embedding'] = [json.dumps(emb) for emb in batch_embeddings]\n",
    "\n",
    "    batch_number = i // batch_size\n",
    "    batch_output_path = os.path.join(output_s3_dir, f'embeddings_batch_{batch_number}.csv')\n",
    "    with fs.open(batch_output_path, 'w') as f_out:\n",
    "        batch_df[['JOB_TITLE', 'embedding']].to_csv(f_out, index=False)\n",
    "\n",
    "    processed_batches.append(i)\n",
    "    with fs.open(checkpoint_file, 'w') as f_chk:\n",
    "        json.dump(processed_batches, f_chk)\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"Processing of embeddings completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808edd4-ce2c-40bf-a3fb-03d710790632",
   "metadata": {},
   "source": [
    "## Unifying Batches into a Single CSV\n",
    "\n",
    "### Overview\n",
    "- **Consolidation**: Combines all batch files with embeddings into one unified CSV file.\n",
    "- **Validation**: Verifies the validity of embeddings during the process.\n",
    "- **Local Merge**: Merges data locally before final upload.\n",
    "- **Output**: Uploads the consolidated file to an S3 bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89b21c-cd96-4bf5-b614-1c49f3215465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "output_s3_dir = 's3://ml-translation-input-dev/embeddings_batches/'\n",
    "final_output_s3_path = 's3://ml-translation-input-dev/job_titles_with_embeddings.csv'\n",
    "local_final_path = '/tmp/job_titles_with_embeddings.csv'\n",
    "def verificar_embedding(embedding_str, dim=1536):\n",
    "    try:\n",
    "        embedding = json.loads(embedding_str)\n",
    "        return isinstance(embedding, list) and len(embedding) == dim\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "batch_files = fs.ls(output_s3_dir)\n",
    "\n",
    "header_written = False\n",
    "\n",
    "with open(local_final_path, 'w', encoding='utf-8') as f_final:\n",
    "    for file in tqdm(batch_files, desc=\"Unificando Batches\"):\n",
    "        if file.endswith('.csv'):\n",
    "            with fs.open(file, 'r') as f_batch:\n",
    "                df_batch = pd.read_csv(f_batch)\n",
    "            \n",
    "            df_batch['valid_embedding'] = df_batch['embedding'].apply(verificar_embedding)\n",
    "            \n",
    "            if not df_batch['valid_embedding'].all():\n",
    "                num_incorrectos = (~df_batch['valid_embedding']).sum()\n",
    "                print(f\"bath {file} has {num_incorrectos} embeddings wrong.\")\n",
    "            \n",
    "            df_batch.drop(columns=['valid_embedding'], inplace=True)\n",
    "            \n",
    "            df_batch.to_csv(f_final, index=False, header=not header_written, mode='w' if not header_written else 'a', encoding='utf-8')\n",
    "            \n",
    "            if not header_written:\n",
    "                header_written = True\n",
    "\n",
    "with open(local_final_path, 'rb') as f_final:\n",
    "    fs.put(local_final_path, final_output_s3_path)\n",
    "\n",
    "print(\"done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909a60b-9acd-4cfa-93b6-5655d0a96e29",
   "metadata": {},
   "source": [
    "## Verifying Correct Length of Embeddings\n",
    "\n",
    "### Overview\n",
    "- **Validation**: Ensures that all embeddings have the expected dimensionality.\n",
    "- **Error Handling**: Identifies and logs any embeddings with incorrect lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53ce57f-a50a-48e5-81d7-56bf0e8c392e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings processed: 1000\n",
      "Embeddings corrects: 1000\n",
      "Embeddings incorrects: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "import json\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "final_output_s3_path = 's3://ml-translation-input-dev/job_titles_with_embeddings.csv'\n",
    "\n",
    "chunksize = 10000 \n",
    "\n",
    "def verificar_embedding(embedding_str, dim=1536):\n",
    "    try:\n",
    "        embedding = json.loads(embedding_str)\n",
    "        return isinstance(embedding, list) and len(embedding) == dim\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "total_embeddings = 0\n",
    "correct_embeddings = 0\n",
    "incorrect_embeddings = 0\n",
    "\n",
    "for chunk in pd.read_csv(fs.open(final_output_s3_path, 'r'), chunksize=chunksize):\n",
    "chunk = pd.read_csv('data_job_title_industry_embeddings.csv')\n",
    "chunk['valid_embedding'] = chunk['embedding'].apply(verificar_embedding)\n",
    "correct = chunk['valid_embedding'].sum()\n",
    "incorrect = (~chunk['valid_embedding']).sum()\n",
    "total_embeddings += len(chunk)\n",
    "correct_embeddings += correct\n",
    "incorrect_embeddings += incorrect\n",
    "\n",
    "print(f\"Total embeddings processed: {total_embeddings}\")\n",
    "print(f\"Embeddings corrects: {correct_embeddings}\")\n",
    "print(f\"Embeddings incorrects: {incorrect_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1c826-b86f-4587-aa32-ff2f3c6cfe84",
   "metadata": {},
   "source": [
    "## Adjusting Cluster Model\n",
    "\n",
    "### Overview\n",
    "- **Purpose**: Incrementally trains a MiniBatchKMeans clustering model on job title embeddings.\n",
    "- **Process**:\n",
    "  - Reads embeddings in chunks from an S3 file.\n",
    "  - Converts and processes embedding data into a numerical matrix.\n",
    "  - Updates the clustering model iteratively using `partial_fit`.\n",
    "- **Output**:\n",
    "  - Saves the trained clustering model locally.\n",
    "  - Uploads the model file to an S3 bucket for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdebeaf-c588-4bf6-bdb2-6b3bb192b5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "final_output_s3_path = 's3://ml-translation-input-dev/job_titles_with_embeddings.csv'\n",
    "\n",
    "chunksize = 5000 \n",
    "\n",
    "num_clusters = 1000\n",
    "kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=42, batch_size=1000)\n",
    "\n",
    "print(\"Adjusting the clustering model with embeddings...\")\n",
    "for chunk in tqdm(pd.read_csv(fs.open(final_output_s3_path, 'r'), chunksize=chunksize)):\n",
    "    embeddings = chunk['embedding'].apply(lambda x: json.loads(x) if isinstance(x, str) else [0.0]*1536)\n",
    "    embeddings_matrix = np.vstack(embeddings.values).astype(np.float32)\n",
    "    kmeans.partial_fit(embeddings_matrix)\n",
    "\n",
    "model_path = 's3://ml-translation-input-dev/minibatch_kmeans_model.pkl'\n",
    "with fs.open(model_path, 'wb') as f_model:\n",
    "    joblib.dump(kmeans, f_model)\n",
    "\n",
    "print(\"Clustering model adjusted and stored in S3.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8d3a0-9e8f-4213-826a-c8a5fcaf8ff4",
   "metadata": {},
   "source": [
    "## Assign Clusters and Sampling\n",
    "\n",
    "### Overview\n",
    "- **Purpose**: Assigns cluster labels to each embedding and performs sampling from the clustered data.\n",
    "- **Process**:\n",
    "  - Loads the trained MiniBatchKMeans model from S3.\n",
    "  - Reads embeddings in chunks, assigns cluster labels using the model, and appends results.\n",
    "  - Samples a specified number of data points (up to 30) from each cluster.\n",
    "- **Output**:\n",
    "  - Saves a sampled dataset with job titles, embeddings, and cluster labels.\n",
    "  - Uploads the final sampled file to an S3 bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6a8e7-ca3d-48d2-9f32-ae93181e8a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "final_output_s3_path = 's3://ml-translation-input-dev/job_titles_with_embeddings.csv'\n",
    "model_path = 's3://ml-translation-input-dev/minibatch_kmeans_model.pkl'\n",
    "s3_output_path = 's3://ml-translation-input-dev/job_titles_embeddings_cluster_15000.csv'\n",
    "\n",
    "chunksize = 5000  \n",
    "\n",
    "with fs.open(model_path, 'rb') as f_model:\n",
    "    kmeans = joblib.load(f_model)\n",
    "\n",
    "print(\"Assigning clusters to each embedding...\")\n",
    "sampled_data = []\n",
    "\n",
    "for chunk in tqdm(pd.read_csv(fs.open(final_output_s3_path, 'r'), chunksize=chunksize)):\n",
    "    embeddings = chunk['embedding'].apply(lambda x: json.loads(x) if isinstance(x, str) else [0.0]*1536)\n",
    "    embeddings_matrix = np.vstack(embeddings.values).astype(np.float32)\n",
    "    \n",
    "    clusters = kmeans.predict(embeddings_matrix)\n",
    "    chunk['Cluster'] = clusters\n",
    "    \n",
    "    sampled_data.append(chunk)\n",
    "\n",
    "df_with_clusters = pd.concat(sampled_data, ignore_index=True)\n",
    "\n",
    "print(\"Sampling from each cluster...\")\n",
    "samples_per_cluster = 30\n",
    "sampled_df = df_with_clusters.groupby('Cluster').apply(\n",
    "    lambda x: x.sample(n=samples_per_cluster, random_state=42) if len(x) >= samples_per_cluster else x\n",
    ").reset_index(drop=True)\n",
    "\n",
    "sampled_df = sampled_df[['JOB_TITLE', 'embedding', 'Cluster']].head(15000)\n",
    "\n",
    "with fs.open(s3_output_path, 'w') as f:\n",
    "    sampled_df.to_csv(f, index=False)\n",
    "\n",
    "print(\"Clustering and sampling process completed. File saved in S3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aaeab8-abce-41c2-9937-f72b8eb4a727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_clusters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43501f26-9c90-4562-852f-714c5af62a33",
   "metadata": {},
   "source": [
    "## Final Verification of Sampled Embeddings\n",
    "\n",
    "### Overview\n",
    "- **Purpose**: Ensures the integrity of the sampled embeddings by verifying their dimensions.\n",
    "- **Process**:\n",
    "  - Reads the sampled dataset from S3.\n",
    "  - Calculates the length of each embedding to check for consistency.\n",
    "  - Outputs the frequency distribution of embedding lengths.\n",
    "- **Output**:\n",
    "  - Displays a summary of embedding length frequencies for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d18b0-6d0d-462a-921c-8e9e05187b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "sampled_s3_path = 's3://ml-translation-input-dev/job_titles_sampled10000.csv' \n",
    "\n",
    "df_sampled = pd.read_csv(fs.open(sampled_s3_path, 'r'))\n",
    "\n",
    "df_sampled['embedding_length'] = df_sampled['embedding'].apply(lambda x: len(json.loads(x)) if isinstance(x, str) else 0)\n",
    "length_frequencies = df_sampled['embedding_length'].value_counts()\n",
    "print(length_frequencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd74dbbf-9625-49ad-b7bd-23c51d681b22",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary\n",
    "- Successfully processed job title data to generate embeddings, cluster them, and sample from the clustered data.\n",
    "- Utilized OpenAI's `text-embedding-ada-002` model for embedding generation.\n",
    "- Applied MiniBatchKMeans for clustering and verified the quality of embeddings throughout the process.\n",
    "- Produced a final sampled dataset with cluster assignments, ensuring data integrity.\n",
    "\n",
    "### Outcomes\n",
    "- Generated embeddings stored in S3.\n",
    "- Trained and saved a clustering model for future use.\n",
    "- Created a balanced, sampled dataset for downstream applications.\n",
    "\n",
    "### Next Steps\n",
    "- Analyze the clusters for insights or patterns.\n",
    "- Leverage the sampled dataset for model training or other analyses.\n",
    "- Refine the process further if additional optimizations are needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3644a10-a3ef-4a79-9944-7abc94ad746b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
